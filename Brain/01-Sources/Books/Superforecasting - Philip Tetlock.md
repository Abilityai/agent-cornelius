by [[Philip Tetlock]], [[Dan Gardner]]
[[Forecast|Forecasting]] [[Prediction]], [[Statistics]], [[AI]], [[Causal Thinking]], [[Scientific thinking]] [[Good Judgment]]

## [[Key Insights]]
1. The book is about methodology to make precise [[Forecast]]s.
2. It talks about [[Bias]]es and tools used by the [[Good Judgment]] group of [[Forecast]] that demonstrated amazing forecasting results, better than US intelligence community.
3. It talks personal traits to become a superforecaster, and building effective forecasting teams. 

### Chapters 1-2

[[An average forecaster is as good as a dart-throwing chimp (chance)]]

[[Forecasts for events on 3+ years are as good as a chance]]

[[Chaos]] - minimal changes in the initial conditions can cause dramatically different results in the future. There are hard limits on predictability. It is directly related to [[Objective Ignorance]]. 

[[Forecasts are rarely validated due to lack of demand]]

[[Human + AI will make better forecasts than either of them separately]]

[[Early medicine was an example of cargo-cult science run by overconfident charlatans]]

[[System 1]] - Automatic system, fully based on [[Pattern Recognition]], created for [[Intuition]] [[Judgment]] and for finding quick answers from available in the quick memory - [[Availability Bias]]. It works by the principle What you see is all there is [[WYSIATI]]
[[System 2]] - deliberate, rational system 

[[People are often making decisions with System 1 and only rationalizing with System 2]]

[[Rationalization is done with the use of Causal Thinking]]

[[Attribute substitution - people replace the question they are asked with a simpler one]]

At the same time Snap [[Judgment]] are good in many cases, they were created by evolution to work well in critical situations. They are very convincing, but wrong in complex situations. 

### Chapter 3 - keeping score
Judging [[Forecast]] is much harder than it seems. A lot of them are vague and not specific. 

Numbers and [[Estimate]]s express opinions. 

However when estimates are made in percentage, and they are then judged to be right or wrong based on the outcome - this is not the right approach, this is a form of [[Resulting]]. That's why when forecasting people and organizations tend to use opaque phrases like "quite likely", "fairly good chance" etc..

[[Forecast is equivalent to Betting, e.g. 90% forecast means 9 to 1 bet]]
[[Gambling and Betting companies are adjusting indexes to match the amount of money on each bet]]

[[Brier Score is a way to measure the accuracy of a probability forecast]]

[[Bad forecasters rely on Causal Thinking, Confirmation of their beliefs and overconfidence]]

The other group - he calls them Foxes, the good predictors, were ready to change and review their opinions more often - just like in [[Think Again - Adam Grant]]. 

The first group is bad because they lock in on one [[perspective]] and keep looking for confirmation evidence for this perspective. 

[[Good forecasters are able to consider alternative views and perspectives]]

[[All models are wrong, but some are useful]]

He also talks about the [[Wisdom of a crowd]] theory, stating that it works because different people have different parts of the puzzle, different ideas on which they base the estimate. They also have errors in their [[Judgment]], but those errors tend to cancel out because they contribute to both sides of the right answer. 

### Chapter 4 - superforecasters
He tells a story of intelligence community not measuring their [[accuracy]], but focusing on the [[DMP]]. They were convinced they found nuclear weapons (WMD) in Iraq, but nothing was found. This was a big deal for the [[USA]]. The biggest error was that they presented their results with [[certainty]], with 100% probability. Which never should be done. 

Superforecasters showed consistent 30% better [[accuracy]] than analysis in intelligence agencies. 

People are not dealing well with [[Randomness]]. If they are asked to predict the coin toss, and by chance they do that successfully - they think they will be able to repeat it. 

[[Ratio of luck and skill can be established based on regression to the mean]]

### Chapter 5 - supersmart
When forecasting, the number people arrive at are usually based on the one that feels right. [[Internal Signal of Judgment Completion]]. 

[[Ignoring base rates is the biggest mistake of forecasters]]

[[It is better to estimate base rates first and specific case rates second when forecasting]]

[[Uncertainty is a fundamental part of the reality, it can not be separated from it]]

[[Meaning is created and supported using Causal Thinking]]

[[Daniel Kahneman]] thinks that [[Probabilistic thinking]] people take the [[Outside view]] even to very personal events, seeing them as random events from multitude of possible ones. 

[[Finding Meaning is good for happiness and bad for forecasting abilities]]

### Chapter 7
The general process looks like:
1. Unpack the question into components
2. Distinguish as sharply as you can between known and unknown, leave no assumptions unchecked
3. Adopt the [[Outside view]] and look at the [[Base Rate]]s 
4. Adopt the [[Inside view]] and see how this is going to change the [[Probability]]
5. Pay attention similarities and differences between your views and those of others. 
6. Combine all the above into a single vision, and provide estimations as precise as possible using [[Probability]]
7. Update [[Forecast]] with probability changes whenever new information appears

This resonates with [[Mediating Assessments Protocol (MAP)]].
Monitor developments of the story using [[Google Alerts]].

Good [[Forecast]]ers update their predictions more often than others. 

New information can cause under reaction and over reaction. 

[[Opinions, Values and beliefs are attached to identity]]

[[Good decision-making and forecasting requires separating Ego and Identity from the decision]]

[[Irrelevant information dilutes relevant and results in less confident judgements]]

[[Bayes theorem]] should be applied when adjusting the probabilities. But most of the super-forecasters use it only intuitively. You don't have to calculate it every time. 

### Chapter 8 - Perpetual Beta
If we believe we are bad at something - we will find confirmation for this and won't try to improve. 

[[Consistency Bias]]

To learn from Failure we need to know we failed, so we need [[Feedback]]. Thus they need to be tracked. Because we can't properly remember our past [[Judgment]], we suffer from [[Hindsight bias]]. 

For super forecasters, [[Belief]] is a hypotheses to be tested, not treasure to be protected. 

### Chapter 9 - Superteams
[[Team]] forecasting dynamics. 

[[Productive Groups need to practice Constructive Conflict]]

[[Team]]s were 23% more accurate than individuals. 

Team should have Open-minded thinking. 
[[Productive Group]] should encourage the Culture of sharing - people need to give more than what they take. 

[[Diversity]] of [[perspective]]s improves [[Productive Group]] results. 

### Chapter 10 - Leader's Dilemma

[[Rational Leader must show confidence only after the decision is made]]

[[Mission Command - decisions should be pushed down the hierarchy]]

### Chapter 11 - are they really so super?
Because of the ease of falling into the [[bias]]es like [[WYSIATI]] and others - any one of the super forecaster can quickly be taken over by [[System 1]] and become ineffective. 
[[Scope insensitivity]] - It is probably related to [[Attribute Substitution]] as people are forgetting about the scope and answering the question about how much they are ready to contribute. 
It often shows up with the Timeframe as a Scope. If someone is asked whether something is going to happen in a month or in a year - you may get the same result. 

After substantial practice, deliberate things from [[System 2]] can be embedded in [[System 1]].  

[[History develops in sudden jumps created by Black Swan events]]

Our minds crave [[certainty]]. 

[[Wrong predictions are in part caused by mixing Normal and Fat Tail Distributions]]

### Epilogue
Checklist for superforecasters
1. Concentrate on questions that are not too easy and not too hard, but in the [[Goldy Lock]] zone. 
2. Break large problems into tractable sub-problems. Split between knowable and unknowable. 
3. Strike the right balance between [[Inside view]] and [[Outside view]]. 
4. Strike the right balance between under-reacting and over-reacting to new evidence when updating the [[Belief]] and [[Base Rate]]s.
5. Look for the clashing causal forces for each problem. 
6. Strive to distinguish as many degrees of [[Uncertainty]] as problem permits, but no more. Meaning that you need to have more dials available than just yes, no and maybe. 
7. Find the right balance between [[Overconfidence effect]] and Under-confidence. 
8. Look for the errors behind mistakes, but beware of [[Hindsight bias]]. Do [[Post-mortem]] on both success and failure of [[Forecast]]. If you guessed right it does not mean your thinking was right. 
9. Bring out the best in others, create [[Productive Group]]s, [[Constructive Conflict]]. 
10. Practice what you learn. That's the only way to learn. It's important to have [[Skin In The Game]]
11. Don't treat commandments as commandments. Guidelines can't be fixed. 