# Agentic AI Shifts Risk from Bad Text to Bad Actions

**Source**: AI Readiness for the Agentic Era: A 2024-2025 Analysis, McKinsey & Company
**Document Type**: Research Report
**Extracted By**: AI (document-insight-extractor agent)
**Extraction Date**: 2025-11-17
**Session**: 2025-11-17 Q4 Webinar AI Research

---

## Core Insight

Agentic AI represents a qualitative leap in risk: a generative AI hallucination produces bad text, but an agentic AI hallucination produces bad actions (e.g., ordering the wrong part, executing incorrect transactions). This fundamental shift requires moving from passive ethical principles to active governance with containment, testing, and kill-switches.

---

## Context & Evidence

McKinsey's "Agentic AI Mesh" framework explicitly addresses this new risk class. When AI systems can take autonomous actions rather than just generate suggestions, hallucinations become operational failures with cascading consequences.

The framework identifies three critical risks unique to autonomous agents:
1. **Potentially Harmful Outputs** - Not just misinformation, but incorrect actions with real-world consequences
2. **Misuse of Tools** - Agents accessing other systems can be exploited for hacking or data extraction
3. **Trust Imbalances** - Both excessive trust (uncritical acceptance) and insufficient trust (hampering adoption) create failure modes

This requires "human-in-the-loop" (HITL) not just for review but for approval and active monitoring.

---

## Connections to Knowledge Base

- [[Trust Imbalance - Both Excessive and Insufficient Trust Create AI Failure]] - The dual trust problem unique to agents
- [[Assessment-First Not Pilot-First - The 2025 AI Readiness Consensus]] - Can't pilot agents without governance
- Related to: The concept that agents are digital organisms with fitness functions
- Related to: AI Constitutional Enforcement as impartial dictator

---

**Tags**: #document-insight #ai-agents #risk-management #governance #autonomous-systems #mckinsey-research
