# Judgment-as-a-Service - AI SLAs Shift Liability Model Beyond Uptime

**Source**: AI Governance and Risk Management: The 2025 State of Practice
**Document Type**: Research Brief
**Extracted By**: AI (document-insight-extractor agent)
**Extraction Date**: 2025-11-17
**Session**: 2025-11-17 Q4 Webinar AI Research

---

## Core Insight

AI vendor contracts are evolving from "Software-as-a-Service" to "Judgment-as-a-Service" by demanding performance guarantees on output quality (hallucination rates, accuracy, bias metrics) rather than just system uptime. This fundamentally shifts liability from customer to vendor for the quality of AI-generated decisions.

---

## Context & Evidence

**The Shift:**
Traditional SaaS SLAs track "99.9% uptime" - the system being available. But an AI system can be "up" and still be operationally useless or reputationally disastrous if it produces biased, inaccurate, or harmful outputs.

**New AI-Specific SLAs Include:**
- Accuracy thresholds
- Hallucination rates
- False positive and false negative rates
- Error rates
- Bias metrics with continuous monitoring

**Why Contracts Became Technical Controls:**
Because it's often impossible to technically audit a vendor's proprietary model for bias or data handling, the only effective control is a legally enforceable contractual one. The contract IS the control mechanism.

**The Liability Revolution:**
By demanding performance guarantees, customers move risk FROM themselves TO the vendor. This creates shared accountability for judgment quality, not just system availability.

---

## Connections to Knowledge Base

- [[Related Insight]]: Fourth-party risk in AI supply chains (opaque model training)
- [[Contrasts With]]: Traditional SaaS uptime-only accountability models
- [[Builds On]]: Recognition that AI outputs carry decision liability
- [[Enables]]: Legal frameworks for AI accountability in production systems

---

**Tags**: #document-insight #ai-governance #vendor-risk #sla #liability #accountability #contracts
