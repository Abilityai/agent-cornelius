2022-11-28 11:00
**Source:** [[Superforecasting - Philip Tetlock]]
**References:** [[Decision Making]]

[[Calibration]] is understanding the quality of a forecaster based on several sequential forecast. For example, in case of predicting weather, you can't say anything about the quality of the forecasted based on one result, but you can judge based on series of results. Forecaster is Overconfident when they are predicting something with over 50% probability, but it happens less than 50% of time. And Under-confident in the opposite case. 
Perfect [[Calibration]] is not enough for perfect accuracy. Another aspect of it is [[Resolution]] - it is when forecaster says it will happen and it does, and vice-versa. 
For example, if forecaster predicts something will happen with 60% probability and it happens 60% of the time - this is an ultra-cautious, but well calibrated forecaster.
If they successfully predict low and high percentages - they are decisive and well-calibrated. 

In a combination, [[Calibration]] and [[Resolution]] create [[Brier Score]] - a numerical measure of the quality of any [[Forecast]]. [[Brier Score]] isÂ **a way to verify the accuracy of a probability forecast**.
0 is the best [[Brier Score]] and 2 is the worst, but really 0.5 is already bad. 
Score depends on predictability of the context. 